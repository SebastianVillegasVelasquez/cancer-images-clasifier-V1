{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cancer Classification Project\n",
        "\n",
        "The project is about of how deep learning and machine learning alghoritms can be used to solve problems that includes machine recognition in images.\n",
        "\n",
        "In this case, this project in only about identify patterns to classify benign or malign cancer in images.\n",
        "\n",
        "This project it is only a college project, the main idea is to share knoledge about the data science and deep learning tecnhiques."
      ],
      "metadata": {
        "id": "0hbLLRhMgNQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the neccesary packages"
      ],
      "metadata": {
        "id": "y8lJT8JIiz5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow matplotlib pandas numpy -q"
      ],
      "metadata": {
        "id": "gPLW2-UJi4Pw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataPreparation class\n",
        "\n",
        "The objetive of the class is to analize and create a balance dataset from the root data that already has a train, test and validate sub-folders."
      ],
      "metadata": {
        "id": "WPQOMRp3kFTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(x, y):\n",
        "    \"\"\"\n",
        "    This function handles the image vector and rescale it and optimize it\n",
        "\n",
        "    :param x: (image dataset)\n",
        "    :param y: (labels dataset)\n",
        "    :return: x\n",
        "    \"\"\"\n",
        "    output = tf.keras.layers.Rescaling(1. / 255)(x)\n",
        "    return output, y"
      ],
      "metadata": {
        "id": "rENtphqDlUCu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset, training = False):\n",
        "    \"\"\"\n",
        "    Receive the dataset and apply the corresponding preprocess.\n",
        "\n",
        "    :param dataset\n",
        "    \"\"\"\n",
        "    dataset = dataset.map(lambda x, y: preprocess_data(x, y))\n",
        "    dataset = dataset.cache()\n",
        "    if training:\n",
        "        dataset = dataset.shuffle(3100)\n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "KLF2612rlQv3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class DataPreparation:\n",
        "    \"\"\"\n",
        "    This class makes semi-automatic the creation of datasets.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 root_folder,\n",
        "                 img_size,\n",
        "                 color_mode,\n",
        "                 batch_size,\n",
        "                 class_mode: str = 'binary',\n",
        "                 shuffle: bool = True,\n",
        "                 seed: Optional[int] = None,\n",
        "                 validation_split: Optional[float] = None,\n",
        "                 subset: Optional[str] = None,\n",
        "                 cache: bool = True,\n",
        "                 prefetch: bool = True,\n",
        "                 recursive: bool = False):\n",
        "\n",
        "        self.root_folder = root_folder\n",
        "        self.img_size = img_size\n",
        "        self.color_mode = color_mode\n",
        "        self.batch_size = batch_size\n",
        "        self.class_mode = class_mode\n",
        "        self.shuffle = shuffle\n",
        "        self.seed = seed\n",
        "        self.validation_split = validation_split\n",
        "        self.subset = subset\n",
        "        self.cache = cache\n",
        "        self.prefetch = prefetch\n",
        "        self.recursive = recursive\n",
        "\n",
        "    def create_datasets_from_subdirectories(self) -> Dict[str, tf.data.Dataset]:\n",
        "        root = Path(self.root_folder)\n",
        "        if not root.exists():\n",
        "            logger.error(\"Root folder does not exist: %s\", self.root_folder)\n",
        "            raise FileNotFoundError(f\"Root folder does not exist: {self.root_folder}\")\n",
        "\n",
        "        subdirectories = [p for p in root.iterdir() if p.is_dir()]\n",
        "        datasets = {}\n",
        "        if not subdirectories:\n",
        "            try:\n",
        "                ds = self.create_dataset(str(root))\n",
        "                datasets[root.name] = ds\n",
        "                logger.info(\"Loaded single dataset from root folder.\")\n",
        "            except Exception as e:\n",
        "                logger.exception(\"Failed to load dataset from root folder: %s\", e)\n",
        "            return datasets\n",
        "\n",
        "        for sub in subdirectories:\n",
        "            path = str(sub)\n",
        "            try:\n",
        "                ds = self.create_dataset(path)\n",
        "                datasets[sub.name] = ds\n",
        "            except Exception as e:\n",
        "                logger.exception(\"Skipping dataset at %s due to error: %s\", path, e)\n",
        "\n",
        "        logger.info('Data normalization done!')\n",
        "        return datasets\n",
        "\n",
        "    def create_dataset(self, path: str) -> tf.data.Dataset:\n",
        "        p = Path(path)\n",
        "        class_dirs = [d for d in p.iterdir() if d.is_dir() and any(f for f in d.rglob('*') if f.is_file())]\n",
        "        classes_count = len(class_dirs)\n",
        "\n",
        "        label_mode = self.class_mode\n",
        "        if classes_count == 0:\n",
        "            label_mode = None\n",
        "            logger.info(\"No class subdirectories found in %s — using label_mode=None\", path)\n",
        "        else:\n",
        "            if self.class_mode == 'binary' and classes_count != 2:\n",
        "                label_mode = 'categorical'\n",
        "                logger.info(\"class_mode='binary' but %d classes found in %s — switching to 'categorical'\", classes_count, path)\n",
        "\n",
        "        try:\n",
        "            dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "                directory=path,\n",
        "                image_size=self.img_size,\n",
        "                color_mode=self.color_mode,\n",
        "                batch_size=self.batch_size,\n",
        "                label_mode=label_mode,\n",
        "                shuffle=self.shuffle,\n",
        "                seed=self.seed,\n",
        "                validation_split=self.validation_split,\n",
        "                subset=self.subset,\n",
        "                follow_links=self.recursive\n",
        "            )\n",
        "            logger.info('Dataset loaded successfully from %s', path)\n",
        "            logger.info('Classes found in dataset: %s', getattr(dataset, 'class_names', None))\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Error loading dataset from %s: %s\", path, e)\n",
        "            raise\n",
        "\n",
        "        try:\n",
        "            is_training = 'train' in path.lower()\n",
        "            dataset = prepare_dataset(dataset, training=is_training) if is_training else prepare_dataset(dataset)\n",
        "        except Exception:\n",
        "            logger.exception(\"prepare_dataset failed for %s, returning raw dataset\", path)\n",
        "\n",
        "        try:\n",
        "            if self.cache:\n",
        "                dataset = dataset.cache()\n",
        "            if self.prefetch:\n",
        "                dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        except Exception:\n",
        "            logger.exception(\"Error applying cache/prefetch for %s — returning dataset without those optimizations\", path)\n",
        "\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "BCm9wW0hkfN5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Model and Sub-Models Architecture\n",
        "\n",
        "This project is designed with a specific architectural vision in mind: to leverage the strengths of different deep learning models through an ensemble approach. The core idea is to create various models, each based on a distinct architecture (such as ResNet, Inception, MobileNet, etc.), but all sharing a common foundation. By combining the predictions of these diverse models, we aim to achieve a more robust and accurate overall prediction than any single model could achieve on its own.\n",
        "\n",
        "Here's a breakdown of the planned classes:\n",
        "\n",
        "*   **Base Model Class:** This serves as the foundational class for all other model instances. It will contain the basic functionalities and parameters common to all models, such as methods for compiling the model, training, evaluation, and potentially saving/loading weights. The other model-specific classes will inherit from this Base Model class, ensuring a consistent interface and reducing code duplication. This class defines the core structure and shared logic for handling the models.\n",
        "\n",
        "*   **EfficientNetV2Instance Class:** This class will inherit from the `Base Model` class. It will specifically implement a model based on the EfficientNetV2 architecture. EfficientNetV2 is known for its good balance between accuracy and efficiency, making it a suitable choice for one of the ensemble members.\n",
        "\n",
        "*   **MobileNetV2Instance Class:** This class will also inherit from the `Base Model` class. It will implement a model based on the MobileNetV2 architecture. MobileNetV2 is optimized for mobile and embedded vision applications, focusing on computational efficiency and low latency. Including it in the ensemble brings diversity in terms of model complexity and performance characteristics.\n",
        "\n",
        "*   **ResNet50V2Instance Class:** Another class inheriting from the `Base Model`. This one will implement a model using the ResNet50V2 architecture. ResNet (Residual Networks) are famous for their ability to train very deep networks effectively, and ResNet50V2 is a widely used variant known for its strong performance on image classification tasks.\n",
        "\n",
        "The ultimate goal is to take advantage of the unique features and learning capabilities of each individual architecture (EfficientNetV2, MobileNetV2, ResNet50V2, etc.) through the ensemble method. This approach allows the project to explore how different model perspectives can be combined to improve the overall performance in classifying benign or malignant cancer in images.\n"
      ],
      "metadata": {
        "id": "8FiZxfUVoZ6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base model"
      ],
      "metadata": {
        "id": "gQO3dYFWw_6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from typing import Union, Optional"
      ],
      "metadata": {
        "id": "pa319PV-4CAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_augmentation(x):\n",
        "\n",
        "    x = keras.layers.RandomFlip('horizontal')(x)\n",
        "    x = keras.layers.RandomRotation(0.1)(x)\n",
        "    x = keras.layers.RandomZoom(0.1)(x)\n",
        "    x = keras.layers.RandomTranslation(0.2, 0.2)(x)\n",
        "    x = keras.layers.RandomContrast(0.2)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6-DDR3jsx147"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union, Optional\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "class BaseModel:\n",
        "\n",
        "    def __init__(self,\n",
        "                 model: Optional[keras.Model] = None,\n",
        "                 optimizer: Optional[Union[str, keras.optimizers.Optimizer]] = None,\n",
        "                 loss: Optional[Union[str, keras.losses.Loss]] = None,\n",
        "                 metrics: Optional[Union[list[Union[str, keras.metrics.Metric]]]] = None,\n",
        "                 epochs: Optional[int] = None,\n",
        "                 learning_rate: Optional[float] = None,\n",
        "                 train_dataset: Optional[object] = None,\n",
        "                 test_dataset: Optional[object] = None,\n",
        "                 validation_dataset: Optional[object] = None,\n",
        "                 task_type: Optional[str] = None,\n",
        "                 debug: Optional[bool] = None,\n",
        "                 img_shape: Optional[tuple] = False,\n",
        "                 layer_name: Optional[str] = None,\n",
        "                 ):\n",
        "\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer or keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.metrics = metrics or ['accuracy']\n",
        "        self.loss = loss\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.learning_rate = 0.01\n",
        "        self.task_type = task_type\n",
        "        self.debug = False\n",
        "        self.layer_name = layer_name\n",
        "        self.units = None\n",
        "        self.epochs = epochs or 20\n",
        "\n",
        "        match self.task_type:\n",
        "            case 'binary':\n",
        "                self.units = 1\n",
        "            case 'categorical':\n",
        "                self.units = 3\n",
        "\n",
        "    def build_model(self, base_model):\n",
        "        if base_model is None:\n",
        "            raise ValueError(\"A base model must be provided.\")\n",
        "        base_model.trainable = False\n",
        "\n",
        "        # Removed index calculation from here\n",
        "        print(f'[debug] building model with trainable base set to False')\n",
        "        self.top_model(base_model=base_model)\n",
        "        # Return None or remove return as index is not calculated here anymore\n",
        "        return None\n",
        "\n",
        "    def get_layer_name_index_to_fine_tuning_at(self, model):\n",
        "        # This method is now primarily used internally by fine_tuning_model\n",
        "        layers = [layer.name for layer in model.layers]\n",
        "        if self.layer_name and self.layer_name in layers:\n",
        "             return layers.index(self.layer_name)\n",
        "        elif self.layer_name:\n",
        "            print(f\"[Warning] Layer name '{self.layer_name}' not found in model layers. Returning -1.\")\n",
        "            return -1 # Or handle error appropriately\n",
        "        else:\n",
        "             print(\"[Warning] layer_name is not set. Returning -1 for fine-tuning index.\")\n",
        "             return -1\n",
        "\n",
        "\n",
        "    def fine_tuning_model(self, start_layer: Union[int, str]):\n",
        "        \"\"\"\n",
        "        Unfreezes layers in the model starting from the specified index or layer name for fine-tuning.\n",
        "\n",
        "        This method sets the `trainable` attribute of layers to True from the given\n",
        "        `start_layer` (index or name) onwards. BatchNormalization layers are kept frozen.\n",
        "\n",
        "        :param start_layer: The index (int) or name (str) of the layer from which\n",
        "                            to start fine-tuning.\n",
        "                            If index is negative, it counts from the end of the layers.\n",
        "                            Layers with an index less than the determined starting index\n",
        "                            will be frozen.\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise RuntimeError(\"Model is not built yet. Call build_model() first.\")\n",
        "\n",
        "        actual_index = -1 # Default to no fine-tuning\n",
        "\n",
        "        if isinstance(start_layer, str):\n",
        "            # Find the index if a layer name is provided\n",
        "            layers = [layer.name for layer in self.model.layers]\n",
        "            try:\n",
        "                actual_index = layers.index(start_layer)\n",
        "                print(f\"[debug] Fine-tuning from layer '{start_layer}' at index: {actual_index}\")\n",
        "            except ValueError:\n",
        "                print(f\"[Error] Layer with name '{start_layer}' not found in the model. Skipping fine-tuning.\")\n",
        "                return # Exit if layer name not found\n",
        "        elif isinstance(start_layer, int):\n",
        "            # Use the provided index (handle negative indices)\n",
        "            if start_layer < 0:\n",
        "                actual_index = len(self.model.layers) + start_layer\n",
        "                if actual_index < 0:\n",
        "                    print(f\"[Warning] Negative index {start_layer} is out of bounds. Will not unfreeze any layers.\")\n",
        "                    return # Exit if index is out of bounds\n",
        "                print(f\"[debug] fine-tuning from layer index: {actual_index} (calculated from negative index {start_layer})\")\n",
        "            else:\n",
        "                actual_index = start_layer\n",
        "                if actual_index >= len(self.model.layers):\n",
        "                    print(f\"[Warning] Index {start_layer} is out of bounds. Will not unfreeze any layers.\")\n",
        "                    return # Exit if index is out of bounds\n",
        "                print(f\"[debug] fine-tuning from layer index: {actual_index}\")\n",
        "        elif start_layer is None:\n",
        "             print(\"[debug] start_layer is None. Skipping fine-tuning.\")\n",
        "             return # No fine-tuning if start_layer is None\n",
        "        else:\n",
        "            print(f\"[Error] Invalid type for start_layer: {type(start_layer)}. Must be int, str, or None.\")\n",
        "            return # Exit for invalid type\n",
        "\n",
        "\n",
        "        # Apply trainability based on actual_index\n",
        "        if actual_index != -1: # Only proceed if a valid index was determined\n",
        "            for i, layer in enumerate(self.model.layers):\n",
        "                if i >= actual_index:\n",
        "                    # Keep BatchNormalization layers frozen during fine-tuning\n",
        "                    if not isinstance(layer, keras.layers.BatchNormalization):\n",
        "                        layer.trainable = True\n",
        "                    else:\n",
        "                         layer.trainable = False # Explicitly set BatchNormalization to False\n",
        "            else:\n",
        "                layer.trainable = False # Ensure layers before the index are frozen\n",
        "\n",
        "\n",
        "    def top_model(self, base_model):\n",
        "        inputs = keras.Input(\n",
        "            shape=(224, 224, 3))\n",
        "        x = image_augmentation(inputs)\n",
        "        encoder_output = base_model(x)\n",
        "        x = keras.layers.GlobalAveragePooling2D()(encoder_output)\n",
        "        x = keras.layers.Dropout(0.5)(x)\n",
        "        x = keras.layers.Dense(256)(x)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.ReLU()(x)\n",
        "        outputs = keras.layers.Dense(self.units, activation='sigmoid')(x)\n",
        "        model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "        self.model = model\n",
        "\n",
        "    def compile(self, learning_rate=None):\n",
        "\n",
        "        if learning_rate is None:\n",
        "            optimizer = self.optimizer\n",
        "        else:\n",
        "            optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=self.loss,\n",
        "            metrics=self.metrics\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "\n",
        "        if not hasattr(self, \"callbacks\") or not self.callbacks:\n",
        "            raise RuntimeError(\"Callbacks must be set before training. Call set_callbacks() first.\")\n",
        "\n",
        "        if self.validation_dataset is not None:\n",
        "            return self.model.fit(\n",
        "                self.train_dataset,\n",
        "                batch_size=32,\n",
        "                epochs=self.epochs,\n",
        "                validation_data=self.validation_dataset,\n",
        "                callbacks=self.callbacks\n",
        "            )\n",
        "        else:\n",
        "            return self.model.fit(\n",
        "                self.train_dataset,\n",
        "                batch_size=32,\n",
        "                epochs=self.epochs,\n",
        "                callbacks=self.callbacks\n",
        "            )\n",
        "\n",
        "    def set_callbacks(self,\n",
        "                      filepath_to_save_model: str,\n",
        "                      tensorboard_log_file: str,\n",
        "                      monitor_param: Optional[str] = 'val_loss',\n",
        "                      callbacks: list[keras.callbacks.Callback] = None,\n",
        "                      early_stopping_patiencie: int = 10,\n",
        "                      reducelr_patiente: int = 5,\n",
        "                      factor: float = 0.5,\n",
        "                      ):\n",
        "        if early_stopping_patiencie <= reducelr_patiente:\n",
        "            raise ('Early Stopping patience callback must have to be greater than'\n",
        "                   'ReduceLROnPlateau patience callback')\n",
        "\n",
        "        self.callbacks = callbacks or [\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                patience=early_stopping_patiencie,\n",
        "                monitor=monitor_param,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            keras.callbacks.ModelCheckpoint(\n",
        "                filepath=filepath_to_save_model,\n",
        "                monitor=monitor_param,\n",
        "                save_best_only=True\n",
        "            ),\n",
        "            keras.callbacks.ReduceLROnPlateau(\n",
        "                patience=reducelr_patiente,\n",
        "                monitor=monitor_param,\n",
        "                factor=factor\n",
        "            ),\n",
        "            keras.callbacks.TensorBoard(\n",
        "                log_dir=tensorboard_log_file,\n",
        "                histogram_freq=1,\n",
        "                write_images=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def show_layer_names(self, model=None):\n",
        "\n",
        "        if model is None:\n",
        "            print(\"[Error] model parameter can not be None\")\n",
        "            return\n",
        "        # Use self.model if no model is passed\n",
        "        model_to_show = model if model is not None else self.model\n",
        "        if model_to_show is None:\n",
        "             print(\"[Error] Model is not built yet.\")\n",
        "             return\n",
        "\n",
        "        for idx, layer in enumerate(model_to_show.layers):\n",
        "            print(f\"{idx}: {layer.name}\")\n",
        "\n",
        "\n",
        "    def execute_model_flow(self):\n",
        "        \"\"\"\n",
        "            Execute the complete training workflow for transfer learning.\n",
        "            The workflow consists of:\n",
        "                1. Building the base transfer learning model.\n",
        "                2. Compiling the model with the default optimizer.\n",
        "                3. Performing initial training (feature extraction stage).\n",
        "                4. Unfreezing selected layers for fine-tuning.\n",
        "                5. Recompiling the model with a lower learning rate (1e-5).\n",
        "                6. Training again (fine-tuning stage).\n",
        "\n",
        "            :return: None\n",
        "            :rtype: None\n",
        "            \"\"\"\n",
        "        # Build model sets base_model trainable to False initially\n",
        "        self.build_model() # Assuming subclasses will provide base_model if needed\n",
        "\n",
        "        self.compile()\n",
        "        print(\"\\nStarting initial training (feature extraction)...\")\n",
        "        self.fit()\n",
        "\n",
        "        # Directly pass self.layer_name to fine_tuning_model\n",
        "        # fine_tuning_model will handle finding the index if it's a string\n",
        "        start_layer_for_fine_tuning = self.layer_name if self.layer_name else None # Use layer_name if set\n",
        "\n",
        "        if start_layer_for_fine_tuning: # Proceed with fine-tuning only if a starting layer is specified\n",
        "            print(f\"\\nStarting fine-tuning from: {start_layer_for_fine_tuning}...\")\n",
        "            # Now call the modified fine_tuning_model with the determined starting layer (name or index)\n",
        "            self.fine_tuning_model(start_layer=start_layer_for_fine_tuning)\n",
        "            # Only recompile and fit if fine-tuning was successfully initiated\n",
        "            if any(layer.trainable for layer in self.model.layers): # Check if any layers were unfrozen\n",
        "                 self.compile(learning_rate=1e-5) # Recompile with lower learning rate for fine-tuning\n",
        "                 self.fit()\n",
        "            else:\n",
        "                 print(\"\\nNo layers were unfrozen for fine-tuning. Skipping fine-tuning training stage.\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nSkipping fine-tuning as no starting layer name was specified in the constructor.\")"
      ],
      "metadata": {
        "id": "RHMsJsXGxBCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet50V2"
      ],
      "metadata": {
        "id": "ZjT-bFP1ycwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet50V2Instance(BaseModel):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs['loss'] = kwargs.get('loss', 'binary_crossentropy')\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        super().set_callbacks(\n",
        "            filepath_to_save_model='model/saved_models/ResNet50V2Instance.keras',\n",
        "            tensorboard_log_file='logs_dir/categorical_model'\n",
        "        )\n",
        "\n",
        "    def build_model(self, model=None):\n",
        "        if model is None:\n",
        "            base_model = keras.applications.ResNet50V2(\n",
        "                include_top=False,\n",
        "                weights='imagenet',\n",
        "                input_shape=(224, 224, 3)\n",
        "            )\n",
        "        else:\n",
        "            base_model = model\n",
        "        super().build_model(base_model=base_model)\n",
        "\n",
        "    def fine_tuning_model(self):\n",
        "        model = self.model\n",
        "        fine_tuning_at = -50\n",
        "        for layer in model.layers[:fine_tuning_at]:\n",
        "            if isinstance(layer, keras.layers.BatchNormalization):\n",
        "                layer.trainable = False\n",
        "            else:\n",
        "                layer.trainable = True\n",
        "\n",
        "        self.model = model"
      ],
      "metadata": {
        "id": "6lCyoUBnygQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MobileNetV2"
      ],
      "metadata": {
        "id": "ILyfTJekylG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "class MobileNetV2Instance(BaseModel):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs['loss'] = kwargs.get('loss', 'binary_crossentropy')\n",
        "        # Set a default layer_name for fine-tuning or allow it to be passed in\n",
        "        # Using 'block_15_expand' as a common starting point for fine-tuning\n",
        "        kwargs['layer_name'] = kwargs.get('layer_name', 'block_15_expand')\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        super().set_callbacks(\n",
        "            filepath_to_save_model='model/saved_models/MobileNetV2Instance.keras',\n",
        "            tensorboard_log_file='logs_dir/categorical_model'\n",
        "        )\n",
        "\n",
        "    def build_model(self, base_model=None): # Changed parameter name from model to base_model\n",
        "        if base_model is None:\n",
        "            base_model = keras.applications.MobileNetV2(\n",
        "                include_top=False,\n",
        "                weights='imagenet',\n",
        "                input_shape=(224, 224, 3)\n",
        "            )\n",
        "        else:\n",
        "            base_model = base_model\n",
        "        # Pass the base_model to the parent's build_model which handles trainable layers\n",
        "        super().build_model(base_model=base_model)"
      ],
      "metadata": {
        "id": "88J5DSc2ypJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientNetV2\n"
      ],
      "metadata": {
        "id": "JDjuooUg4GIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNetV2Instance(BaseModel):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        kwargs['loss'] = kwargs.get('loss', 'binary_crossentropy')\n",
        "        # Set a default layer_name for fine-tuning or allow it to be passed in\n",
        "        kwargs['layer_name'] = kwargs.get('layer_name', 'block6a_expand') # Example layer name for EfficientNetV2B0, adjust as needed\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        super().set_callbacks(\n",
        "            filepath_to_save_model='model/saved_models/EfficientNetV2Instance.keras',\n",
        "            tensorboard_log_file='logs_dir/categorical_model'\n",
        "        )\n",
        "\n",
        "    def build_model(self, base_model=None): # Changed parameter name from model to base_model\n",
        "        if base_model is None:\n",
        "            base_model = keras.applications.EfficientNetV2B0(\n",
        "                include_top=False,\n",
        "                weights='imagenet',\n",
        "                input_shape=(224, 224, 3)\n",
        "            )\n",
        "        else:\n",
        "            base_model = base_model\n",
        "        super().build_model(base_model=base_model)"
      ],
      "metadata": {
        "id": "88eWluPZ4fc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7333e9b9"
      },
      "source": [
        "## Training the Models\n",
        "\n",
        "With the updated `BaseModel` and its subclasses, the training process is streamlined, especially when using the `execute_model_flow` method.\n",
        "\n",
        "1.  **Instantiate your desired model:** Create an instance of the specific model you want to train (e.g., `MobileNetV2Instance`, `ResNet50V2Instance`, `EfficientNetV2Instance`). When creating the instance, you can pass your training, testing, and validation datasets, as well as the `task_type` (`'binary'` or `'categorical'`). You can also specify the `layer_name` in the constructor to define the starting point for fine-tuning during the `execute_model_flow`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the datasets"
      ],
      "metadata": {
        "id": "Rb4eTif5Gmr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/data'\n",
        "\n",
        "dataset = (DataPreparation(root_folder=dataset_path,\n",
        "        img_size=(224, 224),\n",
        "        color_mode='rgb',\n",
        "        batch_size=32,\n",
        "        class_mode='binary',\n",
        "        shuffle=True).\n",
        "    create_datasets_from_subdirectories())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJSnSlpTGq94",
        "outputId": "9a89e8fb-01e6-40a1-924f-576d2332e728"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 390 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "class TaskType(Enum):\n",
        "    BINARY = \"binary\"\n",
        "    CATEGORICAL = \"categorical\"\n",
        "    REGRESSION = \"regression\""
      ],
      "metadata": {
        "id": "OiTxlMemM7QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5a1de19"
      },
      "source": [
        "# Instantiate the MobileNetV2 model\n",
        "# Replace train_dataset, validation_dataset, and test_dataset with your actual datasets\n",
        "# Make sure to have your datasets loaded before this step\n",
        "mobilenet_model = MobileNetV2Instance(\n",
        "    train_dataset=dataset['train'],        # Use the training dataset from DataPreparation\n",
        "    validation_dataset=dataset['validation'],   # Use the validation dataset from DataPreparation\n",
        "    test_dataset=dataset['test'],         # Use the test dataset from DataPreparation\n",
        "    task_type=TaskType.BINARY.value,        # Or 'categorical' depending on your problem\n",
        "    layer_name='block_15_expand',\n",
        "    epochs=10# Specify the layer name to start fine-tuning from\n",
        ")\n",
        "\n",
        "# Now you can execute the training flow\n",
        "# history = mobilenet_model.execute_model_flow() # This line was in the previous cell and caused the error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a3891e6",
        "outputId": "3ef9c801-e9af-425a-c06f-94c15efc4fa1"
      },
      "source": [
        "# Execute the complete training and fine-tuning process\n",
        "history = mobilenet_model.execute_model_flow()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[debug] building model with trainable base set to False\n",
            "\n",
            "Starting initial training (feature extraction)...\n",
            "Epoch 1/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 126ms/step - accuracy: 0.6596 - loss: 0.6436 - val_accuracy: 0.7026 - val_loss: 0.6007 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 111ms/step - accuracy: 0.7141 - loss: 0.5846 - val_accuracy: 0.8000 - val_loss: 0.4713 - learning_rate: 0.0010\n",
            "Epoch 3/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.7097 - loss: 0.5591 - val_accuracy: 0.7821 - val_loss: 0.4520 - learning_rate: 0.0010\n",
            "Epoch 4/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.7436 - loss: 0.5269 - val_accuracy: 0.7769 - val_loss: 0.4568 - learning_rate: 0.0010\n",
            "Epoch 5/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step - accuracy: 0.7345 - loss: 0.5286 - val_accuracy: 0.8077 - val_loss: 0.4409 - learning_rate: 0.0010\n",
            "Epoch 6/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.7465 - loss: 0.5114 - val_accuracy: 0.7974 - val_loss: 0.4522 - learning_rate: 0.0010\n",
            "Epoch 7/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 100ms/step - accuracy: 0.7494 - loss: 0.5154 - val_accuracy: 0.7615 - val_loss: 0.4826 - learning_rate: 0.0010\n",
            "Epoch 8/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 105ms/step - accuracy: 0.7429 - loss: 0.5197 - val_accuracy: 0.7949 - val_loss: 0.4288 - learning_rate: 0.0010\n",
            "Epoch 9/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 106ms/step - accuracy: 0.7629 - loss: 0.4957 - val_accuracy: 0.8026 - val_loss: 0.4249 - learning_rate: 0.0010\n",
            "Epoch 10/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step - accuracy: 0.7699 - loss: 0.4853 - val_accuracy: 0.7897 - val_loss: 0.4409 - learning_rate: 0.0010\n",
            "\n",
            "Starting fine-tuning from: block_15_expand...\n",
            "[Error] Layer with name 'block_15_expand' not found in the model. Skipping fine-tuning.\n",
            "Epoch 1/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 115ms/step - accuracy: 0.7654 - loss: 0.4854 - val_accuracy: 0.7923 - val_loss: 0.4442 - learning_rate: 1.0000e-05\n",
            "Epoch 2/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.7552 - loss: 0.4878 - val_accuracy: 0.7769 - val_loss: 0.4548 - learning_rate: 1.0000e-05\n",
            "Epoch 3/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step - accuracy: 0.7535 - loss: 0.4884 - val_accuracy: 0.7667 - val_loss: 0.4588 - learning_rate: 1.0000e-05\n",
            "Epoch 4/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.7635 - loss: 0.4786 - val_accuracy: 0.7667 - val_loss: 0.4583 - learning_rate: 1.0000e-05\n",
            "Epoch 5/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 101ms/step - accuracy: 0.7542 - loss: 0.4904 - val_accuracy: 0.7718 - val_loss: 0.4605 - learning_rate: 1.0000e-05\n",
            "Epoch 6/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.7641 - loss: 0.4806 - val_accuracy: 0.7692 - val_loss: 0.4597 - learning_rate: 1.0000e-05\n",
            "Epoch 7/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 103ms/step - accuracy: 0.7551 - loss: 0.4878 - val_accuracy: 0.7667 - val_loss: 0.4578 - learning_rate: 5.0000e-06\n",
            "Epoch 8/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 98ms/step - accuracy: 0.7513 - loss: 0.4886 - val_accuracy: 0.7718 - val_loss: 0.4560 - learning_rate: 5.0000e-06\n",
            "Epoch 9/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 100ms/step - accuracy: 0.7688 - loss: 0.4807 - val_accuracy: 0.7744 - val_loss: 0.4551 - learning_rate: 5.0000e-06\n",
            "Epoch 10/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 104ms/step - accuracy: 0.7555 - loss: 0.5106 - val_accuracy: 0.7744 - val_loss: 0.4540 - learning_rate: 5.0000e-06\n"
          ]
        }
      ]
    }
  ]
}